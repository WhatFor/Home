<article>
    <h2>My opinion on AI</h2>
    <ul>
        <li>First published: <time datetime="2025-12-26">2025-12-26</time></li>
        <li>Last updated: <time datetime="2025-12-26">2025-12-26</time></li>
        <li>State: Unrefined</li>
    </ul>
    <hr />
    <p>
        It's important to begin with some background. To understand how I feel
        about AI, you probably need to understand me.
    </p>
    <p>
        I'm lucky that I stumbled into software engineering. I considered a few
        paths after earning an unrelated degree: game design, UX analysis,
        anything that blended technical and art. It just happened that
        developers were in high demand in 2017, and after spending mornings
        learning Javascript and nights working in kitchens, a friend introduced
        me to a company hiring C# developers.
    </p>
    <p>
        I'm not sure if I'd call myself naturally talented at programming, but I
        was endlessly curious, enthusiastic, and hungry to learn. Within a
        couple years, I was comfortable with ASP.NET and was solving real
        problems for real clients - problems that were probably too big given my
        experience. The company was a small start up - I was employee #4. I look
        back on this period as my chance to sink or swim, and credit it for
        where I am now. I was forced to solve all problems, no matter how
        complex. This isn't to say I learned everything myself; I was certainly
        guided by the talented startup founders that hired me - we were all just
        too busy to spend much time to sit around and discuss engineering in any
        formal way.
    </p>
    <p>
        I loved it. No meetings, no ceremony. Just 7 hour days, blasting code. I
        was in a flow state more often than not, wrapping up days with commits
        holding +5000 lines. There was no mentorship programmes or formalised
        code-review processes. I often had to learn what worked and what didn't
        through failure and iteration, based on feeling and past experiences.
    </p>
    <p>
        Even to this day, I love the feeling of success when I sit down with a
        problem, form an idea for a solution - it's architecture and shape - and
        turn that idea into code, hit run, and see it all work. It's feedback,
        directly from the computer, saying "Yeah, those years were worth it.
        You're good at this".
    </p>
    <p>
        These days, I mostly use my enthusiasm for software to guide others. I
        read endlessly and feed back to the rest of the team through mentorship,
        lightning talks and training days. I absolutely love talking about code,
        discussing trade-offs, planning features and helping others foster the
        skills I value.
    </p>
    <p>
        I'd like to break this down into two parts - AI as a tool for software
        engineering, and AI for everything else.
    </p>
    <h3>AI as a software engineering tool</h3>
    <p>
        I joined the free GitHub Copilot technical preview in November 2021 -
        over a year before the launch of ChatGPT. We didn't know it at the time,
        but that was the beginning of AI for many of us.
    </p>
    <p>
        Back then, Copilot was really just a fancy autocomplete. I'd type
        <code>public Task FindUserAsync(int userId)</code> and, based very
        loosely on context from the current file, I might see a function pop up
        after a few seconds. A lot of the time it'd be pretty close to what I
        wanted, and it was fucking magic compared to what I had before.
    </p>
    <p>
        It felt like all the boilerplate I'd write just fizzled away.
        Controllers in .NET could be created in a second, boring models I've
        written a million times before just conjured from nothing. Like a part
        of my brain had been cloned into the machine, a spirit that worked with
        me to guess what I'd want next.
    </p>
    <p>
        But the spirit was a double-edged sword. After a couple of months, the
        'Copilot hover hand' became a real thing, named after the constant
        interruptions to my writing as I wrote a few keywords then sat and
        hovered over the Tab key, waiting for the spirit to suggest what I
        wanted. Often I knew <em>exactly</em> what I wanted to write next, but
        the machine spirit yelled out, always butting in to do it for me, and I
        let it.
    </p>
    <p>
        The realisation that I'd outsourced the parts of programming I enjoyed
        to Copilot was sudden, and I stopped. I wanted back my flow, the agency
        to fumble through a function, the chance to learn and improve. The very
        act of writing, word by word, was something I valued; it was my way of
        expressing my character, my style, my experience. Copilot wiped all of
        that away.
    </p>
    <p>
        I value the <em>process</em> of programming. It's an art, a celebration
        of many years of technical progression as I stand on the shoulders of
        the giants that have come before me, from punch cards to Rust. I feel
        less for the <em>results</em> of programming. Otherwise I'd probably be
        chasing founder/startup culture, looking to make a fuck load of cash
        from selling a SaaS platform. I've always felt that me and AI don't
        agree on this - AI cares only for the results.
    </p>
    <p>
        I've not talked about "agents" or "vibe coding". This is because I've
        not tried them. Perhaps that's foolish, but given how "vibe coding"
        takes the 2021 Copilot experience and turns it up to 11, I'm convinced
        I'd hate it. Shaking the magic word box and hoping it makes the program
        do what I want shares no commonality with the process that made me fall
        in love with programming. Perhaps I'll give it a go and update this when
        I do.
    </p>
    <p>
        Thankfully, my current employer celebrates my AI scepticism. It's nice
        to have a voice on the opposite side of the arena, keeping the hype in
        check. I'm not forced to use Cursor or a similar editor, but I do fear
        the future.
    </p>
    <p>
        This is an appropriate time for a tangent to talk about fear. Online
        discourse about LLM tooling has been exhausting for years, and I feel
        the majority of it stems from fear. The AI hype circles do well to pump
        up uncertainty and fear when it comes to programming. We've put so much
        effort into learning our craft, it's natural to feel threatened. Will we
        all have to change industries and learn something else entirely?
    </p>
    <p>
        It's understandable to reject AI, to bury your head in the sand and pray
        the AI jobpocalypse isn't coming, but those who embrace the shift seek
        comfort in a different way - holding onto the belief that 'the naysayers
        will be left behind; I'm embracing the future, nothing bad will happen
        to me'. It's all fear. At the end of the day, the companies don't care.
        We will all be replaced when it's convenient.
    </p>
    <h3>AI in business and platforms</h3>
    <p>
        I've dabbled in building apps that make use of LLMs. I've integrated
        with the OpenAI API. I spent time building an app that could pull
        ingredient lists from online recipes before building weekly meal plans,
        focusing on common, fresh ingredient reuse to reduce food waste. The
        OpenAI model (o4) did this... okay. It was all too happy to miss
        ingredients, make them up, or get quantities wrong. Often, it felt like
        herding ducks.
    </p>
    <p>
        But I'm just one guy, not particularly interested in AI. Maybe whole
        teams do a better job, or companies with much more cash to throw around.
    </p>
    <p>
        Except... do they? The hype cycle seemingly knows no bounds, where each
        and every business seems to be cramming in an AI feature because
        everyone else is. It's never 'Should we solve this problem with AI?' and
        always 'Find me a problem to solve with AI'. The only time I find I use
        an AI feature is when it's able to help me search technical
        documentation, such as on GitHub, and even then I find it more effective
        to query the LLM to produce an answer but ignore it in favour of
        following the links it suggests to the real documentation. I need
        correct information when trying to do something technical, and in these
        cases LLM output is a liability.
    </p>
    <p>
        The liability extends to internal business processes too. LLM content
        from others, such as stories/tickets or documentation, is actively
        harmful, especially when not disclosed.
    </p>
    <p>
        I've had to dive into odd technical choices in a PR from a team member
        only to find out their decision process was guided by LLM output on a
        ticket. I've questioned product owners about discrepancies in
        requirement documents, left thinking 'Hold on... that's not right?', not
        knowing it was generated by ChatGPT.
    </p>
    <p>
        The issue is down to expectations. Tickets and documents used to be
        written by a person - known to be correct, every point existing for a
        validated reason. When LLMs are involved it's deceptive, going against
        that expectation and catching people out. This is actively destructive.
        If you prove that you're unable to verify LLM output, I can no longer
        trust your written word; I can't take your input at face value, but
        instead have to fact-check and analyse.
    </p>
    <p>
        Lastly, I fear for future developers. I value the experience I've gained
        over the years, the intuitive understanding I have about code. I worry
        others won't have this. Where's the love for the process when everything
        is about the result?
    </p>
    <h3>Everything else</h3>
    <p>
        I love the indie web - I read personal blogs all the time via RSS feeds.
        It feels like one of the final frontiers of a free and open web, a small
        slice that's not locked away in a walled garden by some tech oligarch.
        Small indie sites enable connection, they're personal and intimate, and
        I fear LLM companies threaten all of it. Over the last few years I've
        often asked myself 'What's the point in writing if it's just going to be
        hoovered up by an LLM?', which sums up a concept I've wrestled with -
        LLM scraping.
    </p>
    <p>
        Mass scraping angered me for a long time. The blog posts we write, the
        code we share, the discussions we have. None of it was created with the
        understanding it'd all be consumed by these bots that are now
        threatening us.
    </p>
    <p>
        I've come to terms accepting that human knowledge is something owned by
        everyone and no one. Wikipedia, documentation, blogs - all of it's
        shared and open knowledge. The consumption of all of this is something I
        welcome, for the sake of our progression. The conflict in my mind comes
        from a simple question - are LLMs progression? I'm not convinced that
        the means justify the ends. The state of the AI industry feels like a
        greedy push for the now, disregarding the future; Careless capitalism at
        the expense of everyone and the environment. Is it for the benefit of
        everyone to use our collective knowledge to pump 100s of billions into
        scaling up more compute now to fuel AI, an unproven concept that
        <em>might</em> pay off? More compute, more RAM, more GPUs, more power,
        now, now, now. Is the AI boom progressing us, or is it ballooning our
        current limitations? Could the incomprehensible amount of wealth and
        resources being pumped into current-generation tech to fuel the machine
        be used to research more efficient electricity generation and compute?
        Capitalism isn't pushing us forward, it's digging us deeper.
    </p>
    <p>
        Lastly, I don't have the same thoughts when it comes to AI and the arts.
        The global ingestion of our collective knowledge is something I can see
        benefit in, but the line is drawn at image/video generation. I feel
        sorrow when looking at the endless facsimile of human artistic
        expression. It's a hideous affront to the human condition, the
        production of art, the hone of our crafts. The only redeeming light in
        all of it is the rise of punk, anti-LLM circles, a much needed boost in
        appreciation of human-made art that's been eroded throughout the 21st
        century.
    </p>
    <p>
        To remove any doubts and to condense my waffle, I am anti-AI. From an
        economic or environmental stance, the current AI push is horrific. From
        an artistic point of view - including the art of code - I believe AI is
        a monstrosity that should be resisted.
    </p>
    <p>
        I don't know if I'll reply, but if you want to talk about this try me at
        <object
            class="email"
            width="220px"
            data="/assets/images/svg/em.svg"
            type="image/svg+xml"
        ></object>
    </p>
</article>
