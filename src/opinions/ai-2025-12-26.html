<article>
    <h2>My opinions on AI</h2>
    <ul>
        <li>First published: <time datetime="2025-12-26">2025-12-26</time></li>
        <li>Last updated: <time datetime="2025-12-27">2025-12-27</time></li>
        <li>State: Unrefined</li>
    </ul>
    <hr />
    <h3>Me</h3>
    <p>
        I'm going to start with some personal history. Knowing who I am will add
        context... I think that's important.
    </p>
    <p>
        I stumbled into software engineering. I considered a few paths after
        earning an unrelated degree—web dev, game design, UX analysis—anything
        that combined tech with art. It turned out developers were in high
        demand in 2017 and after spending mornings learning JavaScript I was
        introduced to a company hiring C# developers. I was lucky.
    </p>
    <p>
        Maybe I'm naturally talented at programming (if that's even a thing).
        Regardless, I was always curious and hungry to learn. This made me
        comfortable with ASP.NET after a couple years and I was quickly solving
        real problems. The company was a small start up, and I was employee #4.
        I look on this time as my chance to sink or swim and believe it pushed
        me to grow fast. I was forced to solve any problem that popped up, no
        matter how complex. This isn't to say I learned everything myself; I was
        certainly guided by the talented startup founders that hired me.
    </p>
    <p>
        I loved it. No meetings. No ceremony. Just 7 hour days, blasting code. I
        was in a flow-state most of the time, pushing commits with 1000s of
        lines added. There was no mentorship programme or code-review process; I
        often had to learn what worked and what didn't through failure and
        iteration, based on intuition.
    </p>
    <p>
        I've always loved the feeling when I sit down with a problem to solve
        and dream up a solution—it's architecture, an ambiguous shape—and turn
        that idea into code that works on the first run. It's feedback from the
        computer saying<em>"Yeah, you're good at this"</em>.
    </p>
    <p>
        I mostly guide others these days. I inform our direction, mentor, hold
        lightning talks and organise training days. I love talking about code,
        discussing trade-offs, planning features and helping others build their
        skills.
    </p>
    <hr />
    <br />
    <p>
        I'd like to break this down into two parts. AI as a tool for software
        engineering and AI for everything else.
    </p>
    <br />
    <h3>AI as a software engineering tool</h3>
    <p>
        I joined the free GitHub Copilot preview in November 2021 (over a year
        before the launch of ChatGPT). I didn't know it at the time, but that
        was the beginning of AI in it's current form.
    </p>
    <p>
        Copilot was really just a fancy autocomplete back then. I'd type
        <code style="color: midnightblue">
            'public Task FindUserAsync(int userId)'</code
        >
        and I'd see a full function pop up after a few seconds. A lot of the
        time it'd be pretty close to what I wanted, and it was fucking magic
        compared to what we had before.
    </p>
    <p>
        It felt like boilerplate just fizzled away. Controllers could be created
        in a second. Boring models, written a million times before, summoned
        from nothing. It was like my brain had been cloned into the machine as a
        friendly spirit, working tirelessly to write what I wanted next.
    </p>
    <p>
        But the spirit wasn't so friendly. The
        <em>'Copilot hover hand'</em> quickly became a real thing—named after
        the constant interruptions as I sat and hovered over the tab key,
        waiting for the spirit to make a suggestion. Often I knew
        <em>exactly</em> what I wanted to write but the spirit yelled out,
        always butting in to do it for me. It was a mutany; the spirit wanted to
        play pilot.
    </p>
    <p>
        I had outsourced my favourite parts of programming to Copilot. I wanted
        back my flow. I wanted the agency to fumble through a function, the
        chance to learn and improve, to <em>do</em>. The very act of writing my
        code was something I valued; it was my way of expressing my character
        and my style. Copilot stole that from me.
    </p>
    <p>
        The <em>process</em> of programming is valuable. It's an art; a
        celebration of years of progression as we stand on the shoulders of the
        giants that have come before us. I feel much less for the
        <em>results</em> of coding. I'd probably be chasing founder status
        otherwise, looking to make a fuckload of cash from selling a SaaS
        platform. I've always felt that me and AI don't agree on this—AI cares
        only for results.
    </p>
    <p>
        I haven't talked about <em>agents</em> or <em>vibe coding</em>. I've not
        tried them. Perhaps that's foolish, but given how they take my Copilot
        experience and turn it up to 11 I'm convinced I'd hate it. There's no
        commonality between shaking the magic word box and the coding I fell in
        love with. I'll be back with an update if I even try it.
    </p>
    <!--<p>
        Thankfully, my current employer celebrates my AI scepticism. It's nice
        to have a voice on the opposite side of the arena, keeping the hype in
        check. I'm not forced to use Cursor or a similar editor, but I do fear
        the future.
    </p>-->
    <p>
        I want to talk about fear. Online chat about AI tooling has exhausted me
        for years. The majority of the arguments are rooted in fear. The AI hype
        circles pump up uncertainty and fear about the future of hand-coding.
        We've put in so much effort, we're going to feel threatened. Will we all
        have to change industries and learn something else entirely? It's
        understandable to reject AI—to bury your head in the sand and pray the
        AI jobpocalypse isn't coming—but those who embrace the shift seek
        comfort in a different way, holding onto the belief that
        <em>
            'the naysayers will be left behind; I'm embracing the future,
            nothing bad will happen to me' </em
        >. We'll see.
    </p>
    <h3>AI in business and platforms</h3>
    <p>
        I've built apps that make use of LLMs. I've used the OpenAI API. I've
        launched an app that pulled ingredient lists from recipes to build
        weekly meal plans, focused on reusing fresh ingredients to reduce food
        waste. ChatGPT (o4) did this... okay. It was all too happy to miss
        ingredients, make them up, or get quantities wrong. Often, it felt like
        herding ducks.
    </p>
    <p>
        But I'm just one guy, not particularly interested in AI. Maybe whole
        teams do a better job, or companies with much more cash to throw around.
    </p>
    <p>
        Except... do they? The hype knows no bounds. Every business seems to be
        cramming in an AI feature because... everyone else is? It's never
        <em>'Should we solve this problem with AI?'</em> and always
        <em>'Find me a problem to solve with AI'</em>. To this day, I'm yet to
        see any amount of money and talent build a truly impressive AI feature.
        They're just fast search engines (that sometimes lie).
    </p>
    <p>
        The liability of AI impacts business teams too. LLM content from
        others—such as stories/tickets or documentation—is actively harmful.
        This is <em>especially</em> true when AI use isn't disclosed.
    </p>
    <p>
        I've dug into odd technical choices from team members only to find out
        the decision process was guided by LLM text that snaked it's way onto a
        ticket. I've questioned product owners about requirement documents that
        left me thinking <em>'Hold on... that's not right?'</em>, not knowing it
        was generated by ChatGPT.
    </p>
    <p>
        It's down to expectations. Tickets and documents used to be written by a
        person—known to be correct where every word existed for a reason. If an
        LLM is secretly used, it goes against that expectation and catches
        people out. This is actively destructive. We can no longer trust the
        written word; we can't take someone's input at face value, but instead
        have to fact-check and analyse. AI isn't dangerous because it's bad;
        It's dangerous because it's
        <em>almost</em> good.
    </p>
    <p>
        Lastly, I fear for the future of software. I value the experience I've
        gained over the years, the intuitive understanding I have about code. I
        worry others won't have this. Where's the love for the process when
        everything is about the result?
    </p>
    <h3>Everything else</h3>
    <p>
        I love the indie web. I read personal blogs all the time. It feels like
        one of the final frontiers of a free and open web, a small slice that's
        not locked away in a walled garden by some tech oligarch. Small indie
        sites enable connection. They're personal and intimate, but I fear the
        LLM machines threaten it all. Over the last few years I've struggled
        with a question:
        <em
            >'What's the point in writing if it's just going to be hoovered up
            by an LLM?'</em
        >.
    </p>
    <p>
        Mass scraping of the web angered me for a long time. The blog posts we
        write, the code we share, the discussions we have; None of it was
        created with the understanding it'd be consumed by the bots that now
        threaten us.
    </p>
    <p>
        I've come to accept that our collective knowledge is something owned by
        everyone and no one. Wikipedia, documentation, blogs—It's all shared and
        open. For the sake of our progression, I welcome the use of our
        knowledge. There's a simple question that hangs in my mind, however;
        <em>Are LLMs progression?</em> I'm not convinced the end justifies the
        means.
    </p>
    <p>
        The state of the AI industry feels like a greedy push for the now; a
        disregard of the future; careless capitalism at the expense of everyone
        and the environment.
    </p>
    <p>
        Who benefits from using our collective knowledge to pump 100s of
        billions into scaling up more compute? Serving an unproven concept that
        <em>might</em> pay off.
    </p>
    <p>More compute, more RAM, more GPUs, more power, now, now, now.</p>
    <p>
        Can we call the AI boom progress, or are we ballooning our current
        limitations? Could the incomprehensible amount of wealth and resources
        being pumped into current-gen tech be used to research more efficient
        electricity generation and compute? Capitalism isn't pushing us forward,
        it's digging us deeper.
    </p>
    <p>
        Lastly, I don't have the same thoughts when it comes to AI and art. I
        can see benefit in ingesting knowledge, but I draw a line at image or
        video generation. Looking at the endless facsimile of artistic
        expression is sad. It's an attack on what it means to be human; art and
        the hone of our crafts.
    </p>
    <p>
        To remove any doubts and to condense my waffle, I am anti-AI. From an
        economic or environmental stance; From an artistic point of view; From
        the comfort of my AI-free IDE; AI is a monstrosity. Resist.
    </p>
    <p>
        I don't know if I'll reply, but if you want to talk about this try me at
        <object
            class="email"
            width="235px"
            data="/assets/images/svg/em.svg"
            type="image/svg+xml"
        ></object>
    </p>
</article>
